{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Networks - Deep Learning\n",
        "\n",
        "##Î™ntermediate Assignment- Multiclass Classification using Nearest Neighbor and Nearest Class Centroid Models\n",
        "###Dimitrios Tikvinas AEM: 9998"
      ],
      "metadata": {
        "id": "S6SnS__BvzGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Imports**"
      ],
      "metadata": {
        "id": "aFUnykoWxCC7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "P-XKZgGPvo8I"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectPercentile, chi2\n",
        "\n",
        "import numpy as np\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR-10 dataset (only way found to load them efficiently)\n",
        "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Store the images to be left unattached\n",
        "X_train_un, X_test_un = X_train, X_test\n",
        "\n",
        "# Define class labels as strings for CIFAR-10\n",
        "class_labels = [\n",
        "    'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "]\n"
      ],
      "metadata": {
        "id": "H6KSfGRdx-uN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "0JAjS8Bu3YDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training data consists of *32x32 RGB images* with pixels' values being in the range of [0, 255]. We normalize this range to be [0, 1], due to the preferance of small input values for faster convergence, equal contribution of each pixel in the mix and for better generalization overall"
      ],
      "metadata": {
        "id": "n7DyEZg540Ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to float32 array for easier management\n",
        "X_train, X_test = np.array(X_train, dtype=np.float32), np.array(X_test, dtype=np.float32)\n",
        "\n",
        "#  Normalize pixel values to be between 0 and 1\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0"
      ],
      "metadata": {
        "id": "eRlv74N-3vhg"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each image has 32x32 pixels, with each one having 3 values for each RGB color. To be able to handle this, we will *flatten* the images into 1-D vector\n"
      ],
      "metadata": {
        "id": "ardS9MQ968BQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the images\n",
        "X_train, X_test = X_train.reshape(X_train.shape[0], -1), X_test.reshape(X_test.shape[0], -1)"
      ],
      "metadata": {
        "id": "sV9ZgKjQ67EN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction\n",
        "\n",
        "Here, PCA is used to reduce the number of features (dimensions) in the dataset while retaining as much variance as possible. It transforms the original data into a new set of uncorrelated variables called principal components. We choose as components's percentage the value **0.9**\n"
      ],
      "metadata": {
        "id": "wJHFgRiM7uTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=0.9).fit(X_train)\n",
        "X_train_pca = pca.transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "print(\"We extract {} feautures from the original {}.\".format(X_train_pca.shape[1],X_train.shape[1]))"
      ],
      "metadata": {
        "id": "-kjCI1zH7uiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d26c2aa-79b2-4e06-c94c-8389fd4e8aad"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We extract 99 feautures from the original 3072.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardize features by removing the mean and scaling to unit variance using the StandardScaler from sklearn for robustness and interpretability"
      ],
      "metadata": {
        "id": "d18vdeLzRl8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_pca_scaled = scaler.fit_transform(X_train_pca)\n",
        "X_test_pca_scaled = scaler.transform(X_test_pca)"
      ],
      "metadata": {
        "id": "xZvwXXMrphT6"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Selection\n",
        "\n",
        "Using a criterion (such as information gain, chi square test or other statistical criteria) we can eliminate the redundant features and keep only the important ones. In this case, we use the chi square test and keep only a percentage of the features with the best scores."
      ],
      "metadata": {
        "id": "ycbXNkl1Nr70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_perc = SelectPercentile(chi2,percentile=60).fit(X_train, Y_train)\n",
        "X_train_feat_sel = features_perc.transform(X_train)\n",
        "X_test_feat_sel = features_perc.transform(X_test)\n",
        "print(\"We extract {} feautures from the original {}.\".format(X_train_feat_sel.shape[1],X_train.shape[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJockqUiNsMN",
        "outputId": "94497e44-b8ab-4b99-a176-dc169793ed02"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We extract 1843 feautures from the original 3072.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1-Nearest Neighbors Classifier"
      ],
      "metadata": {
        "id": "UgWowjb9m8s_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of nearest neighbors\n",
        "k = 1\n",
        "\n",
        "# Initialize the K-Nearest Neighbors classifier\n",
        "knn_1 = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "# Start the timer\n",
        "start = time.time()\n",
        "\n",
        "# Train the K-Nearest Neighbors model on the training data\n",
        "knn_1.fit(X_train_pca, Y_train.ravel())\n",
        "\n",
        "# Stop the timer\n",
        "end = time.time()\n",
        "\n",
        "print(\"Training time: {}s\\n\".format(end-start))\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Make predictions on the test data\n",
        "Y_pred = knn_1.predict(X_test_pca)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(\"Testing time: {}s\\n\".format(end-start))\n",
        "\n",
        "# Calculate and store the accuracy of the model on both train and test set\n",
        "accuracy_train = accuracy_score(knn_1.predict(X_train_pca), Y_train)\n",
        "accuracy_test = accuracy_score(Y_pred, Y_test)\n",
        "\n",
        "print(f'Accuracy on train set: {accuracy_train:.2f}')\n",
        "print(f'Accuracy on test set: {accuracy_test:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChiJqDGCY0bE",
        "outputId": "923fd9d2-5c53-4835-da02-7ba95a0ca3af"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 0.014717817306518555s\n",
            "\n",
            "Testing time: 5.956500291824341s\n",
            "\n",
            "Accuracy on train set: 1.00\n",
            "Accuracy on test set: 0.39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the kNN algorithm has a small training time but a rather big evaluation-on-the-test-set time, reaching the accuracy of the Nearest Class Centroid model."
      ],
      "metadata": {
        "id": "nVFOmA6AYXdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3-Nearest Neighbors Classifier"
      ],
      "metadata": {
        "id": "8YD_e09J5ZGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of nearest neighbors\n",
        "k = 3\n",
        "\n",
        "# Initialize the K-Nearest Neighbors classifier\n",
        "knn_3 = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "# Start the timer\n",
        "start = time.time()\n",
        "\n",
        "# Train the K-Nearest Neighbors model on the training data\n",
        "knn_3.fit(X_train_pca, Y_train.ravel())\n",
        "\n",
        "# Stop the timer\n",
        "end = time.time()\n",
        "\n",
        "print(\"Training time: {}s\\n\".format(end-start))\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Make predictions on the test data\n",
        "Y_pred = knn_3.predict(X_test_pca)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(\"Testing time: {}s\\n\".format(end-start))\n",
        "\n",
        "# Calculate and store the accuracy of the model on both train and test set\n",
        "accuracy_train = accuracy_score(knn_3.predict(X_train_pca), Y_train)\n",
        "accuracy_test = accuracy_score(Y_pred, Y_test)\n",
        "\n",
        "print(f'Accuracy on train set: {accuracy_train:.2f}')\n",
        "print(f'Accuracy on test set: {accuracy_test:.2f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "ylMhLuwh5ZtW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "159a8eb1-85c0-4a32-fb38-f0c6090ddad1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 0.006695985794067383s\n",
            "\n",
            "Testing time: 4.830379247665405s\n",
            "\n",
            "Accuracy on train set: 0.62\n",
            "Accuracy on test set: 0.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, adding more neighbors really doesn't help us since the performance is actually worse than that of the 1-NN model and the evaluation time is bigger."
      ],
      "metadata": {
        "id": "5AXReGtrYKdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nearest Class Centroid Classifier\n"
      ],
      "metadata": {
        "id": "Eo9KUR6wnFwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Nearest Centroid classifier\n",
        "ncc = NearestCentroid()\n",
        "\n",
        "# Start the timer\n",
        "start = time.time()\n",
        "\n",
        "# Train the Nearest Centroid model on the training data\n",
        "ncc.fit(X_train_pca_scaled, Y_train.ravel())\n",
        "\n",
        "# Stop the timer\n",
        "end = time.time()\n",
        "\n",
        "print(\"Training time: {}s\\n\".format(end-start))\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Make predictions on the test data\n",
        "Y_pred = ncc.predict(X_test_pca_scaled)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(\"Testing time: {}s\\n\".format(end-start))\n",
        "\n",
        "# Calculate and store the accuracy of the model on both train and test set\n",
        "accuracy_train = accuracy_score(ncc.predict(X_train_pca_scaled), Y_train)\n",
        "accuracy_test = accuracy_score(Y_pred, Y_test)\n",
        "\n",
        "print(f'Accuracy on train set: {accuracy_train:.2f}')\n",
        "print(f'Accuracy on test set: {accuracy_test:.2f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d59UjkHrqM9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f213a179-051b-4bcf-e1eb-36e9a49ddff6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 0.01882338523864746s\n",
            "\n",
            "Testing time: 0.01365351676940918s\n",
            "\n",
            "Accuracy on train set: 0.40\n",
            "Accuracy on test set: 0.40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The benefit of this algorithm is its really fast training and evaluation time. It ended up having the highest accuracy on the test set"
      ],
      "metadata": {
        "id": "6pL2SqxbX-Ey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusions\n",
        "\n",
        "On the tables given below we gathered the accuracy on the test data, the training and the testing durations of each of the 3 models presented above for different cases of data preprocessing\n",
        "1. 1-Nearest Neighbors\n",
        "\n",
        "Data Processing Technique  | Accuracy Score | Training time | Testing Time\n",
        "-------------------|------------------|-------|-------\n",
        "Raw training       | 0.35 | 0.0984s  | 146.5682s\n",
        "With Normalization       | 0.35 | 0.0980s | 135.9891s\n",
        "With PCA 0.9 | 0.39 | 0.0313s  | 6.3179s\n",
        "With SelectPecentile 60% | 0.34 |0.5854s |80.5707s\n",
        "With One-Hot Encoding, PCA | 0.39 | 0.0716s  | 11.0615s\n",
        "With Standard Scaler, PCA | 0.33 | 0.0127s | 6.5207s\n",
        "\n",
        "2. 3-Nearest Neighbors\n",
        "\n",
        "Data Processing Technique  | Accuracy Score | Training time | Testing Time\n",
        "-------------------|------------------|-------|-------\n",
        "Raw training       | 0.33 | 0.0822s | 129.7064s\n",
        "With Normalization       | 0.33 | 0.0896s  | 135.6514s\n",
        "With PCA 0.9 | 0.37 | 0.0120s | 5.3075s\n",
        "With SelectPecentile 60% | 0.32 |0.5844s | 95.9935s\n",
        "With One-Hot Encoding, PCA | 0.31 |  0.0539s | 11.0070s\n",
        "With Standard Scaler, PCA | 0.30 | 0.0150s | 5.4616s\n",
        "\n",
        "\n",
        "3. Nearest Class Centroid\n",
        "\n",
        "Data Processing Technique  | Accuracy Score | Training time | Testing Time\n",
        "-------------------|------------------|-------|-------\n",
        "Raw training       | 0.28 | 0.5726s  | 0.4469s\n",
        "With Normalization       | 0.28 | 0.3470s  | 0.3088s\n",
        "With PCA 0.9 | 0.28 | 0.0260s | 0.0089s\n",
        "With SelectPecentile 60% | 0.27 | 1.4529s | 0.4553s\n",
        "With One-Hot Encoding, PCA | 0.28 | 0.0230s | 0.0087s\n",
        "With Standard Scaler, PCA | 0.40 | 0.0246s | 0.0110s\n",
        "\n",
        "  As we can see, each model's accuracy remains pretty bad in the range 30-40%,\n",
        "even after implementing every data preprocessing method mentioned above. The dimensionality reduction accomplished by PCA reduced the prediction time in the Nearest Neighbors' Classifiers by 95%!!!, which can be easily understood by the methodology of PCA.\n",
        "\n",
        "We end up with choosing for the 1- and 3 - Nearest Neighbors Classifiers *Normalization* and *PCA 0.9* and for the Nearest Centroid Classifier *Normalization*, *PCA 0.9* and *Standard Scaler*"
      ],
      "metadata": {
        "id": "W4YLc5r8mYtu"
      }
    }
  ]
}